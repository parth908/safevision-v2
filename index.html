<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <title>SafeVision V2</title> <!-- Title for the new app -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <!-- Manifest for PWA features -->
    <link rel="manifest" href="manifest.json">
    <meta name="theme-color" content="#000000">
    <link rel="apple-touch-icon" href="icon-192.png">

    <!-- Font Awesome for Icons (Microphone for listening status) -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

    <style>
        /* Ensure html and body take full viewport dimensions */
        html, body {
            margin: 0;
            padding: 0;
            width: 100vw;
            height: 100vh;
            overflow: hidden; /* Prevent any scrolling */
        }
        body {
            font-family: sans-serif;
            background-color: #000;
            color: #fff;
            display: flex;
            justify-content: center;
            align-items: center;
            position: relative; /* Needed for absolute positioning of children */
        }
        /* Video and Canvas elements to cover the entire screen */
        video, canvas {
            position: absolute;
            top: 0; left: 0;
            width: 100vw; height: 100vh;
            object-fit: cover; /* Ensures video fills screen, may crop if aspect ratios differ */
            z-index: 0; /* Behind other UI elements */
            border: 2px solid transparent; 
        }
        video.ready {
            border: 2px solid limegreen; /* Green border when video is ready */
        }

        /* Information overlay for status, weather, datetime, object detection */
        #info {
            position: absolute;
            top: 10px; left: 10px;
            text-align: left; /* Align text to left for readability */
            background-color: rgba(0, 0, 0, 0.7);
            padding: 10px;
            border-radius: 8px;
            z-index: 2; /* Above video/canvas */
            font-size: 0.9em;
        }
        /* Styling for warning text (e.g., "Person is very close") */
        .warning-text { color: red; font-weight: bold; }
        /* Status display for speech recognition (e.g., "Listening...") */
        #speechRecognitionStatus {
            position: absolute;
            top: 10px; right: 10px;
            background-color: rgba(0,0,0,0.7);
            padding: 8px 12px;
            border-radius: 8px;
            color: yellow;
            font-size: 0.9em;
            display: flex;
            align-items: center;
            gap: 5px;
            z-index: 3;
        }
        #speechRecognitionStatus .fas {
            animation: pulse 1.5s infinite;
        }
        @keyframes pulse {
            0% { transform: scale(1); opacity: 1; }
            50% { transform: scale(1.1); opacity: 0.8; }
            100% { transform: scale(1); opacity: 1; }
        }

        /* Styles for the call simulation screenshot overlay */
        #callSimulationOverlay {
            /* Force hidden on load and only shown by JS */
            display: none !important; 
            position: fixed;
            top: 0; left: 0;
            width: 100vw; height: 100vh;
            background-color: rgba(0,0,0,0.98); /* Near-black background for true full screen effect */
            display: flex; /* Will be set to 'flex' by JS when visible */
            justify-content: center;
            align-items: center;
            z-index: 9999; /* Ensure it's on top of everything */
            cursor: pointer; /* Indicates it's clickable */
        }

        #callScreenshot {
            width: 100vw; /* Make image fill the entire viewport width */
            height: 100vh; /* Make image fill the entire viewport height */
            object-fit: cover; /* Crop if necessary to cover the entire screen */
            border-radius: 0; /* No rounded corners for true fullscreen */
            box-shadow: none; /* No shadow for true fullscreen */
            display: block; /* Ensure it behaves like a block element */
        }

        /* Utility class to forcefully hide elements */
        .hidden {
            display: none !important;
        }
        /* New debug info elements */
        #debugInfo {
            position: absolute;
            bottom: 10px; left: 10px;
            background-color: rgba(0, 0, 0, 0.7);
            padding: 10px;
            border-radius: 8px;
            z-index: 4;
            font-size: 0.8em;
            color: #0f0; /* Green for debug info */
            text-align: left;
        }
        /* Visual indicator for detection activity */
        #detectionActivity {
            position: absolute;
            bottom: 10px; right: 10px;
            width: 15px;
            height: 15px;
            background-color: red; /* Default to red (inactive) */
            border-radius: 50%;
            z-index: 4;
            opacity: 0.5;
        }
        #detectionActivity.active {
            background-color: limegreen; /* Green when active */
            animation: blink 1s infinite alternate;
        }
        @keyframes blink {
            from { opacity: 1; }
            to { opacity: 0.3; }
        }
    </style>
</head>
<body>
    <video id="video" class="ui-element" autoplay playsinline muted></video>
    <canvas id="canvas" class="ui-element"></canvas>
    <div id="info" class="ui-element">
        <div id="status">üîÑ Loading AI model...</div>
        <div id="weather">üå°Ô∏è Temp: 35¬∞C | üå¨Ô∏è Wind: 15 km/h | Sunny</div> 
        <div id="datetime"></div>
        <div id="object">üëÅÔ∏è Waiting for detection...</div>
        <div id="proximityStatus" style="color: limegreen;">üü¢ No person detected</div>
    </div>
    <div id="speechRecognitionStatus" class="ui-element">
        <i class="fas fa-microphone"></i> Listening...
    </div>

    <!-- New Debug Info Display -->
    <div id="debugInfo" class="ui-element">
        <div>State: <span id="debugState">IDLE</span></div>
        <div>Speaking: <span id="debugSpeaking">false</span></div>
        <div>Muted: <span id="debugMuted">false</span></div>
        <div>Video Ready: <span id="debugVideoReady">false</span></div>
        <div>Model Loaded: <span id="debugModelLoaded">false</span></div>
        <div>Voices (Native): <span id="debugVoiceCount">0</span></div> <!-- Native voices -->
        <div>ResponsiveVoice: <span id="debugResponsiveVoiceStatus">Loading...</span></div> <!-- New ResponsiveVoice status -->
    </div>

    <!-- Detection Activity Indicator -->
    <div id="detectionActivity"></div>

    <!-- Call Simulation Overlay -->
    <div id="callSimulationOverlay">
        <!-- IMPORTANT: Ensure 'call-112-screenshot.png' is uploaded to your GitHub repo's root directory -->
        <img id="callScreenshot" src="call-112-screenshot.png" alt="Emergency Call Simulation">
    </div>

    <!-- TensorFlow.js and COCO-SSD Model Libraries from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
    
    <!-- ResponsiveVoice.js for fallback/primary TTS -->
    <!-- IMPORTANT: Replace 'YOUR_API_KEY' with your actual ResponsiveVoice API Key if you have one. 
         For non-commercial/testing use, you might be able to use a default or get a free one from responsivevoice.org -->
    <script src="https://code.responsivevoice.org/responsivevoice.js?key=YOUR_API_KEY"></script> 

    <script>
        // --- DOM Elements ---
        const video = document.getElementById("video");
        const canvas = document.getElementById("canvas"); 
        const ctx = canvas.getContext("2d");
        const statusEl = document.getElementById("status");
        const weatherEl = document.getElementById("weather");
        const datetimeEl = document.getElementById("datetime");
        const objectEl = document.getElementById("object");
        const proximityStatusEl = document.getElementById("proximityStatus");
        const speechRecognitionStatusEl = document.getElementById("speechRecognitionStatus");
        const callSimulationOverlay = document.getElementById('callSimulationOverlay');
        // New debug info elements
        const debugStateEl = document.getElementById('debugState');
        const debugSpeakingEl = document.getElementById('debugSpeaking');
        const debugMutedEl = document.getElementById('Muted'); // Corrected ID
        const debugVideoReadyEl = document.getElementById('debugVideoReady');
        const debugModelLoadedEl = document.getElementById('debugModelLoaded');
        const debugVoiceCountEl = document.getElementById('debugVoiceCount'); 
        const debugResponsiveVoiceStatusEl = document.getElementById('debugResponsiveVoiceStatus'); // New element
        const detectionActivityEl = document.getElementById('detectionActivity'); 

        // Group all UI elements that should hide during simulation
        const uiElementsToHide = document.querySelectorAll('.ui-element');

        // --- Variables ---
        let model, recognition, fullInfoIntervalId;
        let speaking = false;
        let isMuted = false;
        let lastSpokenFullInfo = "";
        const SPEECH_COOLDOWN_FULL_INFO = 60000; // 60 seconds (1 minute) cooldown for periodic info updates
        const SPEECH_COOLDOWN_WARNING = 3000; // 3 seconds cooldown for proximity warnings
        let lastFullInfoSpeechTime = 0; // Timestamp of last full info speech
        let lastWarningSpeechTime = 0; // Timestamp of last warning speech
        const CALL_SIMULATION_DURATION = 3000; // 3 seconds for screenshot display
        let callSimulationTimeoutId; // Timeout ID for the call simulation auto-hide
        
        // State management for speech recognition and Gemini processing
        let currentRecognitionState = 'IDLE'; // States: 'IDLE', 'PROCESSING_GEMINI', 'PROCESSING_GEMINI_ASSISTANT'

        // --- Debug UI Update Function ---
        function updateDebugInfo() {
            debugStateEl.textContent = currentRecognitionState;
            debugSpeakingEl.textContent = speaking;
            debugMutedEl.textContent = isMuted;
            debugVideoReadyEl.textContent = video.readyState >= 2; 
            debugModelLoadedEl.textContent = !!model; 
            debugVoiceCountEl.textContent = speechSynthesis.getVoices().length; 
            debugResponsiveVoiceStatusEl.textContent = typeof responsiveVoice !== 'undefined' && responsiveVoice.enabled() ? 'Ready' : 'Not Ready'; // Update ResponsiveVoice status
            console.log(`[Debug Info Update] State: ${currentRecognitionState}, Speaking: ${speaking}, Muted: ${isMuted}, Video Ready: ${video.readyState >= 2}, Model Loaded: ${!!model}, Voices (Native): ${speechSynthesis.getVoices().length}, ResponsiveVoice: ${debugResponsiveVoiceStatusEl.textContent}`);
        }

        // --- Service Worker ---
        if ('serviceWorker' in navigator) {
            window.addEventListener('load', () => {
                console.log('[Service Worker Debug] Attempting to register service worker.');
                console.log('[Service Worker Debug] Current protocol:', location.protocol);
                const swPath = 'service-worker.js';
                console.log('[Service Worker Debug] Service Worker script path:', swPath);

                if (location.protocol === 'https:' || location.protocol === 'http:') {
                    try {
                        navigator.serviceWorker.register(swPath)
                            .then(registration => {
                                console.log('[Service Worker] registered! Scope:', registration.scope);
                            })
                            .catch(err => {
                                console.error('[Service Worker] registration failed:', err);
                                console.error('[Service Worker] Full error object:', err);
                            });
                    } catch (e) {
                        console.error('[Service Worker] Direct registration call failed:', e);
                    }
                } else {
                    console.warn('[Service Worker] Not registering service worker: Unsupported protocol for Service Workers:', location.protocol);
                }
            });
        }

        // --- Date & Time ---
        function updateDateTime() {
            const now = new Date();
            datetimeEl.textContent = `üìÖ ${now.toDateString()} | ‚è∞ ${now.toLocaleTimeString([], { hour: '2-digit', minute: '2-digit', second: '2-digit' })}`;
        }
        setInterval(updateDateTime, 1000); updateDateTime(); 

        // --- Camera Setup ---
        async function setupCamera() {
            console.log('[Camera Debug] Attempting to set up camera...');
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: { facingMode: 'environment', width: { ideal: 640 }, height: { ideal: 480 } },
                    audio: false
                });
                video.srcObject = stream;
                return new Promise(resolve => {
                    video.onloadedmetadata = () => {
                        console.log('[Camera Debug] Video metadata loaded. Playing video.');
                        video.play();
                        video.classList.add('ready'); 
                        resizeCanvas();
                        updateDebugInfo(); 
                        resolve(video);
                    };
                });
            } catch (error) {
                console.error("Error accessing camera:", error);
                statusEl.textContent = `Camera access denied or not available. (${error.name})`; 
                statusEl.style.color = 'red';
                updateDebugInfo(); 
                return null;
            }
        }
        function resizeCanvas() {
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            console.log(`[Canvas Debug] Canvas resized to: ${canvas.width}x${canvas.height}`);
        }

        // --- Object Detection ---
        const FRAMES_TO_SKIP = 2; 
        const PROXIMITY_THRESHOLD_HEIGHT = 450;
        async function detectFrame() {
            detectionActivityEl.classList.add('active'); // Indicate activity
            
            if (video.readyState < 2) { 
                console.log('[DetectFrame Debug] Video not ready (state:', video.readyState, '). Skipping frame.');
                detectionActivityEl.classList.remove('active'); 
                requestAnimationFrame(detectFrame); 
                return; 
            }
            
            if (callSimulationOverlay.style.display !== 'none') { 
                console.log('[DetectFrame Debug] In call simulation. Skipping frame.');
                detectionActivityEl.classList.remove('active'); 
                requestAnimationFrame(detectFrame); 
                return; 
            }

            let predictions;
            try {
                if (model) {
                    predictions = await model.detect(video);
                    console.log('[DetectFrame Debug] Model.detect() called. Predictions count:', predictions.length); 
                } else { 
                    console.log('[DetectFrame Debug] Model not loaded yet. Skipping frame.');
                    detectionActivityEl.classList.remove('active'); 
                    requestAnimationFrame(detectFrame); 
                    return; 
                }
            } catch (e) { 
                console.error("Error during model detection:", e);
                detectionActivityEl.classList.remove('active'); 
                requestAnimationFrame(detectFrame); 
                return; 
            }

            tf.tidy(() => { 
                ctx.clearRect(0, 0, canvas.width, canvas.height); 
                
                let personDetected = false;
                let detectedObjectsText = "No objects detected."; 
                window.detectedObjectsForGemini = []; 

                predictions.forEach(pred => {
                    if (pred.score > 0.6) {
                        const [x, y, width, height] = pred.bbox;
                        ctx.strokeStyle = "#00FF00"; ctx.lineWidth = 2; ctx.strokeRect(x, y, width, height);
                        ctx.fillStyle = "#00FF00"; ctx.font = '16px sans-serif';
                        ctx.fillText(`${pred.class} (${Math.round(pred.score * 100)}%)`, x, y > 10 ? y - 5 : 10);
                        
                        detectedObjectsText = `I see a ${pred.class}`; 
                        window.detectedObjectsForGemini.push({ 
                            class: pred.class, score: pred.score, bbox: { x, y, width, height }
                        });

                        if (pred.class === 'person') {
                            personDetected = true;
                            if (height > PROXIMITY_THRESHOLD_HEIGHT) {
                                ctx.strokeStyle = 'red'; ctx.lineWidth = 4; ctx.strokeRect(x, y, width, height);
                                ctx.fillStyle = 'red'; ctx.fillText(`VERY CLOSE!`, x, y + height + 20);
                                speak("Warning: Person very close.", 'warning');
                            }
                        }
                    }
                });
                objectEl.textContent = detectedObjectsText;
                proximityStatusEl.textContent = personDetected ? "Person detected" : "No person detected"; 
            }); 

            requestAnimationFrame(detectFrame);
        }

        // --- Speech Synthesis ---
        function stripEmojis(text) {
            return text.replace(/(\u00a9|\u00ae|[\u2000-\u3300]|\ud83c[\ud000-\udfff]|\ud83d[\ud000-\udfff]|\ud83e[\ud000-\udfff])/g, '');
        }

        async function speak(text, type = 'fullInfo') {
            const cleanedText = stripEmojis(text);
            console.log(`[TTS Debug] speak() called with text: "${cleanedText}", type: "${type}"`);
            updateDebugInfo();

            if (isMuted && type !== 'emergency') { 
                console.log(`[TTS Debug] Speech (${type}) blocked: Muted.`);
                return;
            }
            
            if (type !== 'emergency' && (speaking || (Date.now() - (type === 'warning' ? lastWarningSpeechTime : lastFullInfoSpeechTime) < (type === 'warning' ? SPEECH_COOLDOWN_WARNING : SPEECH_COOLDOWN_FULL_INFO)))) {
                console.log(`[TTS Debug] Speech (${type}) blocked due to cooldown or already speaking. Speaking: ${speaking}, Last Full Info: ${lastFullInfoSpeechTime}, Last Warning: ${lastWarningSpeechTime}`);
                return;
            }

            // Cancel any ongoing speech before starting new one
            if (speechSynthesis.speaking) {
                console.log("[TTS Debug] speechSynthesis.speaking is true. Cancelling current speech.");
                speechSynthesis.cancel();
            }
            if (typeof responsiveVoice !== 'undefined' && responsiveVoice.speaking()) {
                console.log("[TTS Debug] responsiveVoice.speaking is true. Cancelling current speech.");
                responsiveVoice.cancel();
            }

            speaking = true; // Set speaking flag early
            updateDebugInfo();

            // --- Attempt ResponsiveVoice first ---
            if (typeof responsiveVoice !== 'undefined' && responsiveVoice.enabled()) {
                console.log("[TTS Debug] Attempting to speak using ResponsiveVoice.");
                try {
                    // ResponsiveVoice doesn't have direct onstart/onend promises, use callbacks
                    responsiveVoice.speak(cleanedText, "US English Female", {
                        onstart: () => { console.log("[TTS Debug] ResponsiveVoice speech started."); },
                        onend: () => {
                            speaking = false;
                            if (type === 'fullInfo') lastFullInfoSpeechTime = Date.now();
                            else if (type === 'warning') lastWarningSpeechTime = Date.now();
                            updateDebugInfo();
                            console.log("[TTS Debug] ResponsiveVoice speech ended.");
                        },
                        onerror: (e) => {
                            console.error("[TTS Debug] ResponsiveVoice error:", e);
                            speaking = false;
                            updateDebugInfo();
                            // Fallback to native if ResponsiveVoice fails
                            console.log("[TTS Debug] ResponsiveVoice failed, falling back to native SpeechSynthesis.");
                            tryNativeSpeech(cleanedText, type);
                        }
                    });
                    return; // ResponsiveVoice is handling it
                } catch (e) {
                    console.error("[TTS Debug] Error calling ResponsiveVoice.speak():", e);
                    // Fall through to native speech if ResponsiveVoice call itself fails
                }
            } else {
                console.warn("[TTS Debug] ResponsiveVoice not available or not enabled. Falling back to native SpeechSynthesis.");
            }

            // --- Fallback to Native SpeechSynthesis ---
            function tryNativeSpeech(textToSpeak, originalType) {
                const voices = speechSynthesis.getVoices();
                console.log(`[TTS Debug] Native SpeechSynthesis: Found ${voices.length} voices.`);
                if (!voices.length) {
                    console.warn("[TTS Debug] Native SpeechSynthesis: No voices available. Cannot speak.");
                    statusEl.textContent = "Speech not available (no voices, check device settings)"; 
                    statusEl.style.color = 'red';
                    speaking = false;
                    updateDebugInfo();
                    return;
                }

                const utterance = new SpeechSynthesisUtterance(textToSpeak);
                utterance.lang = "en-IN";
                const indianVoice = voices.find(voice => voice.lang === 'en-IN' || voice.lang === 'en-GB' || voice.name.includes('India'));
                if (indianVoice) {
                    utterance.voice = indianVoice;
                    console.log("[TTS Debug] Native SpeechSynthesis: Using voice:", indianVoice.name);
                } else {
                    console.warn("[TTS Debug] Native SpeechSynthesis: No specific Indian/GB English voice found, using default voice:", voices[0]?.name || 'N/A');
                    utterance.voice = voices[0]; 
                }

                utterance.rate = 1.0; utterance.pitch = 1.0; utterance.volume = 1.0;
                
                utterance.onstart = () => { 
                    speaking = true; 
                    updateDebugInfo(); 
                    console.log("[TTS Debug] Native Speech started for: " + textToSpeak); 
                };
                utterance.onend = () => {
                    speaking = false;
                    if (originalType === 'fullInfo') lastFullInfoSpeechTime = Date.now();
                    else if (originalType === 'warning') lastWarningSpeechTime = Date.now();
                    updateDebugInfo(); 
                    console.log("[TTS Debug] Native Speech ended for: " + textToSpeak);
                };
                utterance.onerror = (event) => { 
                    console.error('[TTS Debug] Native SpeechSynthesisUtterance.onerror:', event.error, "for text:", textToSpeak, event); 
                    speaking = false; 
                    updateDebugInfo(); 
                };
                console.log("[TTS Debug] Calling native speechSynthesis.speak().");
                speechSynthesis.speak(utterance);
            }

            // If ResponsiveVoice was attempted and didn't return, or failed, try native
            if (!(typeof responsiveVoice !== 'undefined' && responsiveVoice.enabled())) {
                tryNativeSpeech(cleanedText, type);
            }
        }

        function speakFullInformation() {
            console.log('[SpeakFullInfo Debug] Attempting to speak full information.');
            if (currentRecognitionState === 'IDLE' && callSimulationOverlay.style.display !== 'flex') { 
                const combinedText = `
                    Status: ${statusEl.textContent}.
                    Weather: ${weatherEl.textContent}.
                    Date and Time: ${datetimeEl.textContent}.
                    Object detection: ${objectEl.textContent}.
                    Proximity status: ${proximityStatusEl.textContent}.
                `.replace(/\s+/g, ' ').trim();
                if (combinedText !== lastSpokenFullInfo) {
                    speak(combinedText, 'fullInfo');
                    lastSpokenFullInfo = combinedText;
                } else {
                    console.log('[SpeakFullInfo Debug] Text is the same as last time, skipping speech.');
                }
            } else {
                console.log(`[SpeakFullInfo Debug] Not speaking: currentRecognitionState is ${currentRecognitionState} or in call simulation.`);
            }
        }

        // --- Speech Recognition ---
        function setupSpeechRecognition() {
            console.log('[SR Debug] Setting up Speech Recognition.');
            const SpeechRecognitionAPI = window.SpeechRecognition || window.webkitSpeechRecognition;

            if (!SpeechRecognitionAPI) {
                speechRecognitionStatusEl.innerHTML = "Voice commands not supported by your browser."; 
                speechRecognitionStatusEl.style.color = 'red';
                console.warn("Web Speech API (SpeechRecognition) not supported by this browser.");
                recognition = null; 
                return;
            }
            
            recognition = new SpeechRecognitionAPI(); 
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = 'en-IN';

            recognition.onstart = () => { 
                console.log('[SR Debug] Recognition started.'); 
                speechRecognitionStatusEl.innerHTML = `<i class="fas fa-microphone"></i> Listening...`;
            };
            recognition.onend = () => {
                console.log('[SR Debug] Recognition ended.');
                if (currentRecognitionState === 'IDLE' && callSimulationOverlay.style.display !== 'flex' && recognition.continuous) {
                    try { 
                        console.log('[SR Debug] Attempting to restart recognition.');
                        recognition.start(); 
                    } catch (e) { 
                        console.warn('[SR Debug] Failed to restart recognition:', e); 
                    }
                } else {
                    console.log(`[SR Debug] Not restarting recognition: currentRecognitionState is ${currentRecognitionState} or in call simulation.`);
                }
            };
            recognition.onerror = (event) => {
                console.error('[SR Debug] Recognition error:', event.error);
                if (event.error === 'not-allowed') {
                    speak("Microphone permission denied. Voice commands are disabled.", 'emergency'); 
                    speechRecognitionStatusEl.innerHTML = "Mic permission denied!"; 
                    speechRecognitionStatusEl.style.color = 'red';
                    recognition.continuous = false; 
                } else if (event.error === 'no-speech') {
                    console.log('[SR Debug] No speech detected.');
                }
            };

            recognition.onresult = async (event) => {
                const results = event.results;
                for (let i = event.resultIndex; i < results.length; i++) {
                    const transcript = results[i][0].transcript.toLowerCase().trim();
                    const isFinal = results[i].isFinal;

                    if (!isFinal) {
                        speechRecognitionStatusEl.innerHTML = `<i class="fas fa-microphone"></i> Listening: ${transcript}...`;
                        continue;
                    }

                    console.log('[SR Debug] Final transcript:', transcript);

                    if (isFinal && currentRecognitionState === 'IDLE') {
                        if (transcript.includes("mute")) {
                            isMuted = true;
                            await speak("Muted.", 'emergency'); 
                            speechRecognitionStatusEl.innerHTML = `<i class="fas fa-microphone"></i> Muted | Listening for commands...`;
                            updateDebugInfo();
                        } else if (transcript.includes("unmute")) {
                            isMuted = false;
                            await speak("Unmuted.", 'emergency'); 
                            speechRecognitionStatusEl.innerHTML = `<i class="fas fa-microphone"></i> Listening for commands...`;
                            updateDebugInfo();
                        } else if (transcript.includes("describe scene") || transcript.includes("what do you see")) {
                             getSceneDescriptionFromGemini();
                        } else if (transcript.includes("emergency")) {
                            await speak(`Initiating simulated emergency call. For an actual direct emergency call, quickly press your phone's power button five times.`, 'emergency');
                            showCallSimulation();
                        } else {
                            await askGemini(transcript);
                        }
                    }
                }
            };
        }

        function startContinuousRecognition() {
            console.log('[SR Debug] Calling startContinuousRecognition. Current state:', currentRecognitionState);
            if (recognition && currentRecognitionState === 'IDLE') { 
                try {
                    recognition.continuous = true;
                    recognition.interimResults = true;
                    recognition.start();
                    speechRecognitionStatusEl.innerHTML = '<i class="fas fa-microphone"></i> Listening for commands...';
                    console.log('[SR Debug] Continuous recognition started.');
                } catch (e) {
                    console.warn('[SR Debug] Recognition start failed:', e);
                }
            } else {
                console.log('[SR Debug] Recognition not started: recognition object not available or state not IDLE.');
            }
        }

        // --- Gemini API Integration for Scene Description ---
        async function getSceneDescriptionFromGemini() {
            if (currentRecognitionState !== 'IDLE') {
                console.log("[Gemini] Skipping description request, app is busy.");
                return;
            }
            currentRecognitionState = 'PROCESSING_GEMINI';
            updateDebugInfo(); 
            clearInterval(fullInfoIntervalId); 
            if (recognition) { recognition.stop(); } 
            speechSynthesis.cancel(); 
            if (typeof responsiveVoice !== 'undefined') responsiveVoice.cancel(); // Also cancel ResponsiveVoice
            
            speechRecognitionStatusEl.innerHTML = 'Generating description...'; 
            await speak("Generating scene description, please wait.", 'emergency'); 

            try {
                const objects = window.detectedObjectsForGemini || [];
                let promptText = "Describe the following scene based on the detected objects. Make it concise and helpful for a visually impaired user. If no objects are present, say that the view is clear.\n\nDetected objects: ";

                if (objects.length === 0) {
                    promptText += "None.";
                } else {
                    const objectDescriptions = objects.map(obj => {
                        const { x, y, width, height } = obj.bbox;
                        let position = '';
                        const centerX = canvas.width / 2; const centerY = canvas.height / 2;
                        const objectCenterX = x + width / 2; const objectCenterY = y + height / 2;

                        if (width > canvas.width * 0.7 || height > canvas.height * 0.7) position = 'very large, likely close,';
                        else if (width > canvas.width * 0.3 || height > canvas.height * 0.3) position = 'large,';
                        else position = 'small,';

                        if (objectCenterX < centerX * 0.7) position += ' to the left';
                        else if (objectCenterX > centerX * 1.3) position += ' to the right';
                        else position += ' in the center';

                        if (objectCenterY < centerY * 0.7) position += ' (upper part)';
                        else if (objectCenterY > centerY * 1.3) position += ' (lower part)';

                        return `${position} ${obj.class}`;
                    });
                    promptText += objectDescriptions.join('; ') + ".";
                }
                
                console.log("[Gemini] Prompt sent:", promptText);

                const chatHistory = [{ role: "user", parts: [{ text: promptText }] }];
                const payload = { contents: chatHistory };
                const apiKey = ""; 
                const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;

                const response = await fetch(apiUrl, {
                    method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(payload)
                });
                const result = await response.json();

                let geminiDescription = "Could not generate description.";
                if (result.candidates && result.candidates.length > 0 && result.candidates[0].content && result.candidates[0].content.parts && result.candidates[0].content.parts.length > 0) {
                    geminiDescription = result.candidates[0].content.parts[0].text;
                } else {
                    console.error("[Gemini] Unexpected API response structure:", result);
                    if (result.error && result.error.message) geminiDescription = `API Error: ${result.error.message.substring(0, 50)}...`;
                }
                
                console.log("[Gemini] Received description:", geminiDescription);
                await speak(geminiDescription, 'emergency'); 

            } catch (error) {
                console.error("[Gemini] Error generating scene description:", error);
                await speak("Failed to get scene description. Please check internet connection.", 'emergency'); 
            } finally {
                setTimeout(() => {
                    currentRecognitionState = 'IDLE';
                    updateDebugInfo(); 
                    startContinuousRecognition();
                    clearInterval(fullInfoIntervalId); 
                    fullInfoIntervalId = setInterval(speakFullInformation, SPEECH_COOLDOWN_FULL_INFO);
                    console.log("[SR Debug] Full info speaking resumed after Gemini.");
                }, 1000); 
            }
        }

        // --- Gemini API Integration for Basic Assistant ---
        async function askGemini(userQuestion) {
            if (currentRecognitionState !== 'IDLE') {
                console.log("[Gemini Assistant] Skipping request, app is busy.");
                return;
            }

            currentRecognitionState = 'PROCESSING_GEMINI_ASSISTANT'; 
            updateDebugInfo(); 
            clearInterval(fullInfoIntervalId); 
            if (recognition) {
                recognition.stop(); 
            }
            speechSynthesis.cancel(); 
            if (typeof responsiveVoice !== 'undefined') responsiveVoice.cancel(); // Also cancel ResponsiveVoice

            speechRecognitionStatusEl.innerHTML = 'Thinking...'; 
            await speak("Thinking.", 'emergency'); 

            try {
                let chatHistory = [];
                chatHistory.push({ role: "user", parts: [{ text: userQuestion }] }); 

                const payload = { contents: chatHistory };
                const apiKey = ""; 
                const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;

                const response = await fetch(apiUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });
                const result = await response.json();

                let geminiResponse = "I'm sorry, I couldn't process that. Please try again.";
                if (result.candidates && result.candidates.length > 0 &&
                    result.candidates[0].content && result.candidates[0].content.parts &&
                    result.candidates[0].content.parts.length > 0) {
                    geminiResponse = result.candidates[0].content.parts[0].text;
                } else {
                    console.error("[Gemini Assistant] Unexpected API response structure:", result);
                    if (result.error && result.error.message) {
                        geminiResponse = `API Error: ${result.error.message.substring(0, 50)}...`;
                    }
                }
                
                console.log("[Gemini Assistant] Received response:", geminiResponse);
                await speak(geminiResponse, 'emergency'); 

            } catch (error) {
                console.error("[Gemini Assistant] Error asking Gemini:", error);
                await speak("I encountered an error. Please check your internet connection.", 'emergency'); 
            } finally {
                setTimeout(() => {
                    currentRecognitionState = 'IDLE';
                    updateDebugInfo(); 
                    startContinuousRecognition();
                    clearInterval(fullInfoIntervalId); 
                    fullInfoIntervalId = setInterval(speakFullInformation, SPEECH_COOLDOWN_FULL_INFO);
                    console.log("[SR Debug] Full info speaking resumed after Gemini assistant.");
                }, 1000); 
            }
        }

        // --- Call Simulation Functions ---
        function showCallSimulation() {
            uiElementsToHide.forEach(el => el.classList.add('hidden'));
            callSimulationOverlay.style.display = 'flex'; 
            console.log(`[Simulation] Call simulation started for ${CALL_SIMULATION_DURATION / 1000} seconds.`);
            callSimulationTimeoutId = setTimeout(hideCallSimulation, CALL_SIMULATION_DURATION);
        }

        function hideCallSimulation() {
            clearTimeout(callSimulationTimeoutId); 
            callSimulationOverlay.style.display = 'none'; 
            console.log("[Simulation] Call simulation ended.");
            uiElementsToHide.forEach(el => el.classList.remove('hidden'));
            startContinuousRecognition(); 
            clearInterval(fullInfoIntervalId); 
            fullInfoIntervalId = setInterval(speakFullInformation, SPEECH_COOLDOWN_FULL_INFO); 
            console.log("[Simulation] App functionalities resumed.");
        }

        callSimulationOverlay.addEventListener('click', hideCallSimulation);


        // --- Initial App Load Function ---
        async function main() {
            console.log('[Main] App starting...');
            statusEl.textContent = "Loading AI model... SafeVision"; 
            updateDebugInfo(); 
            
            // Wait for ResponsiveVoice to be ready
            console.log('[Main] Waiting for ResponsiveVoice to be ready...');
            await new Promise(resolve => {
                // ResponsiveVoice has its own ready callback
                if (typeof responsiveVoice !== 'undefined' && responsiveVoice.enabled()) {
                    console.log("[Main] ResponsiveVoice is immediately ready.");
                    updateDebugInfo();
                    resolve();
                } else if (typeof responsiveVoice !== 'undefined') {
                    // If ResponsiveVoice script loaded but not enabled yet, wait for its internal ready state
                    const checkResponsiveVoice = setInterval(() => {
                        if (responsiveVoice.enabled()) {
                            console.log("[Main] ResponsiveVoice became ready.");
                            clearInterval(checkResponsiveVoice);
                            updateDebugInfo();
                            resolve();
                        }
                    }, 100); // Check every 100ms
                    setTimeout(() => {
                        if (!responsiveVoice.enabled()) {
                            console.warn("[Main] ResponsiveVoice not ready after 5s timeout. Proceeding without it.");
                            updateDebugInfo();
                            resolve();
                        }
                    }, 5000); // 5 second timeout for ResponsiveVoice
                } else {
                    console.warn("[Main] ResponsiveVoice script not loaded. Proceeding without it.");
                    updateDebugInfo();
                    resolve(); // Resolve immediately if script isn't there
                }
            });
            console.log('[Main] ResponsiveVoice ready status checked. Proceeding.');

            // Also wait for native SpeechSynthesis voices (for fallback)
            console.log('[Main] Waiting for Native SpeechSynthesis voices (for fallback)...');
            await new Promise(resolve => {
                speechSynthesis.onvoiceschanged = () => {
                    console.log("[Main] Native Voices loaded via onvoiceschanged event. Count:", speechSynthesis.getVoices().length);
                    updateDebugInfo(); 
                    resolve();
                };
                if (speechSynthesis.getVoices().length > 0) {
                    console.log("[Main] Native Voices available immediately. Count:", speechSynthesis.getVoices().length);
                    updateDebugInfo(); 
                    resolve();
                } else {
                    setTimeout(() => {
                        if (speechSynthesis.getVoices().length === 0) {
                            console.warn("[Main] Native Voices not loaded after 2s timeout. SpeechSynthesis might not work.");
                            statusEl.textContent = "Native Speech not ready (no voices)"; 
                            statusEl.style.color = 'orange';
                        }
                        updateDebugInfo(); 
                        resolve();
                    }, 2000); 
                }
            });
            console.log('[Main] Native SpeechSynthesis voices ready status checked. Final voice count:', speechSynthesis.getVoices().length);
            updateDebugInfo(); 

            console.log('[Main] Setting up camera...');
            await setupCamera();
            if (!video.srcObject) {
                console.error('[Main] Camera setup failed. Exiting main function.');
                return; 
            }
            console.log('[Main] Camera setup complete.');

            console.log('[Main] Loading COCO-SSD model...');
            model = await cocoSsd.load({base: 'lite_mobilenet_v2'});
            statusEl.textContent = "AI model loaded!"; 
            updateDebugInfo(); 
            console.log("[Main] AI model loaded. Initializing speech recognition.");

            console.log('[Main] Attempting to speak "Welcome to SafeVision."');
            setTimeout(async () => {
                await speak("Welcome to SafeVision. Your visual assistant is ready.", 'fullInfo'); // More descriptive welcome
                console.log('[Main] "Welcome to SafeVision" spoken (or attempted).');

                console.log('[Main] Setting up Speech Recognition...');
                setupSpeechRecognition();
                console.log('[Main] Starting continuous recognition...');
                startContinuousRecognition();
                
                console.log('[Main] Starting periodic full information speaking...');
                speakFullInformation(); 
                fullInfoIntervalId = setInterval(speakFullInformation, SPEECH_COOLDOWN_FULL_INFO);
                console.log("[Main] Periodic full information speaking started.");
                
                console.log('[Main] Starting object detection loop...');
                detectFrame(); 
                console.log('[Main] App initialization complete.');
            }, 1000); // 1 second delay for initial welcome speech, allowing ResponsiveVoice to load fully
        }

        document.addEventListener('DOMContentLoaded', main);
    </script>
</body>
</html>
